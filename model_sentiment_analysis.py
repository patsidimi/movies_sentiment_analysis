!pip install --upgrade transformers

!pip install gradio

!pip install wordcloud

import pandas as pd
import numpy as np
import gradio as gr
import matplotlib.pyplot as plt
import nltk
import os
import re
import seaborn as sns
import spacy
import sys
import subprocess
import torch
import base64
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
from datasets import Dataset
from google.colab import drive
from wordcloud import WordCloud
from collections import Counter
from graphviz import Digraph

drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/Athinorama_movies_dataset.csv'
df = pd.read_csv(file_path, encoding='utf-8')
df.head()

# Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… Î±ÏÎ¹Î¸Î¼Î¿Ï Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½ Î±Î½Î¬ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±
samples_per_category = 6000 // 6

print(df.head())
print("\nÎ Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚:\n")
print("-" * 50)
print(df.info())

# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± missing values
print("\nÎ Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î± missing values:\n")
print("-" * 50)
print(df.isnull().sum())

# ÎšÏÎ±Ï„Î¬Ï‰ Ï‡ÏÎ®ÏƒÎ¹Î¼ÎµÏ‚ ÏƒÏ„Î®Î»ÎµÏ‚
df = df[['review', 'stars']]

# Î•Î¾Î¬Î³Ï‰ Ï„Î¿Î½ Î±ÏÎ¹Î¸Î¼ÏŒ Î±Ï€ÏŒ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿
df['stars'] = df['stars'].str.extract(r'(\d+)').astype(int)

# Î¤ÏƒÎµÎºÎ¬ÏÏ‰ Ï„Î± Ï€ÏÏÏ„Î± Î´ÎµÎ¯Î³Î¼Î±Ï„Î±
df.head()

# ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î·Ï‚ ÎºÎ±Ï„Î±Î½Î¿Î¼Î®Ï‚ Ï„Ï‰Î½ Î²Î±Î¸Î¼Î¿Î»Î¿Î³Î¹ÏÎ½
plt.figure(figsize=(10, 6))
sns.countplot(x='stars', data=df,palette="viridis")
plt.title('ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¹ÏÎ½ Î¤Î±Î¹Î½Î¹ÏÎ½')
plt.xlabel('Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¯Î±')
plt.ylabel('Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î£Ï‡Î¿Î»Î¯Ï‰Î½')
plt.show()

#Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… dataset Î±Î½Î¬ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± ÎºÎ±Î¹ Ï„Ï…Ï‡Î±Î¯Î± Î´ÎµÎ¹Î³Î¼Î±Ï„Î¿Î»Î·ÏˆÎ¯Î±

categories = df['stars'].unique()

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎµÎ½ÏŒÏ‚ ÎºÎµÎ½Î¿Ï DataFrame Î³Î¹Î± Ï„Î± ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î± Î´ÎµÎ¯Î³Î¼Î±Ï„Î±
balanced_df = pd.DataFrame()

# Î“Î¹Î± ÎºÎ¬Î¸Îµ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±, ÎµÏ€Î¹Î»Î­Î³Ï‰ Ï„Î¿Î½ Î¯Î´Î¹Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½
for category in categories:
    # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Ï„Î¿Ï… DataFrame Î³Î¹Î± Ï„Î·Î½ Ï„ÏÎ­Ï‡Î¿Ï…ÏƒÎ± ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±
    category_df = df[df['stars'] == category]

    # Î¤Ï…Ï‡Î±Î¯Î± ÎµÏ€Î¹Î»Î¿Î³Î® Ï„Î¿Ï… Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î¿Ï… Î±ÏÎ¹Î¸Î¼Î¿Ï Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½ Ï‡Ï‰ÏÎ¯Ï‚ ÎµÏ€Î±Î½Î±Ï„Î¿Ï€Î¿Î¸Î­Ï„Î·ÏƒÎ·
    category_sample = category_df.sample(n=samples_per_category, random_state=42)

    # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Ï„Ï‰Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Ï‰Î½ Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½ ÏƒÏ„Î¿ Ï„ÎµÎ»Î¹ÎºÏŒ DataFrame
    balanced_df = pd.concat([balanced_df, category_sample])

# Î‘Î½Î±ÎºÎ¬Ï„ÎµÎ¼Î± Ï„Ï‰Î½ Î³ÏÎ±Î¼Î¼ÏÎ½ Ï„Î¿Ï… Ï„ÎµÎ»Î¹ÎºÎ¿Ï dataset
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Î•Ï€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ· Ï„Î·Ï‚ Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î¯Î±Ï‚ Ï„Ï‰Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¹ÏÎ½
category_counts = balanced_df['stars'].value_counts()
print("ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎºÎ±Ï„Î·Î³Î¿ÏÎ¹ÏÎ½ ÏƒÏ„Î¿ Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î¿ dataset:")
print(category_counts)

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î½Î­Î¿Ï… Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î¿Ï… dataset
balanced_df.to_csv('balanced_dataset.csv', index=False)

# ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î·Ï‚ ÎºÎ±Ï„Î±Î½Î¿Î¼Î®Ï‚ Ï„Ï‰Î½ Î²Î±Î¸Î¼Î¿Î»Î¿Î³Î¹ÏÎ½
plt.figure(figsize=(10, 6))
sns.countplot(x='stars', data=balanced_df,palette="viridis")
plt.title('ÎšÎ±Ï„Î±Î½Î¿Î¼Î® Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¹ÏÎ½ Î¤Î±Î¹Î½Î¹ÏÎ½')
plt.xlabel('Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¯Î±')
plt.ylabel('Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î£Ï‡Î¿Î»Î¯Ï‰Î½')
plt.show()

"""Î ÏÎ¿ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Î½Î± Î±Î½Î±Î³Î½Ï‰ÏÎ¯ÏƒÎµÎ¹ ÎºÎ±Î»ÏÏ„ÎµÏÎ± Î±ÏÎ½Î®ÏƒÎµÎ¹Ï‚, Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ Ï„Î± Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·"""

negation_data = {
    "review": [
        # Î˜ÎµÏ„Î¹ÎºÎ­Ï‚ ÎºÏÎ¹Ï„Î¹ÎºÎ­Ï‚ Î¼Îµ Î¬ÏÎ½Î·ÏƒÎ· (5 Î±ÏƒÏ„Î­ÏÎ¹Î±)
        "Î”ÎµÎ½ Î­Ï‡Ï‰ Î´ÎµÎ¹ ÎºÎ±Î»ÏÏ„ÎµÏÎ· Ï„Î±Î¹Î½Î¯Î± Ï†Î­Ï„Î¿Ï‚.",
        "Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï‚ Î·Î¸Î¿Ï€Î¿Î¹ÏŒÏ‚ Î±Ï€ÏŒ Î±Ï…Ï„ÏŒÎ½.",
        "Î¤Î¯Ï€Î¿Ï„Î± Î´ÎµÎ½ ÏƒÏ…Î³ÎºÏÎ¯Î½ÎµÏ„Î±Î¹ Î¼Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ Î±ÏÎ¹ÏƒÏ„Î¿ÏÏÎ³Î·Î¼Î±.",
        "ÎšÎ±Î¼Î¯Î± Ï„Î±Î¹Î½Î¯Î± Î´ÎµÎ½ Î¼Îµ Î­Ï‡ÎµÎ¹ ÏƒÏ…Î³ÎºÎ¹Î½Î®ÏƒÎµÎ¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿.",

        # Î˜ÎµÏ„Î¹ÎºÎ­Ï‚ ÎºÏÎ¹Ï„Î¹ÎºÎ­Ï‚ Î¼Îµ Î¬ÏÎ½Î·ÏƒÎ· (4 Î±ÏƒÏ„Î­ÏÎ¹Î±)
        "Î”ÎµÎ½ ÎµÎ¯Ï‡Î± Î´ÎµÎ¹ Ï„ÏŒÏƒÎ¿ ÎºÎ±Î»Î® Ï„Î±Î¹Î½Î¯Î± ÎµÎ´Ï ÎºÎ±Î¹ ÎºÎ±Î¹ÏÏŒ.",
        "Î”ÎµÎ½ Ï€ÎµÏÎ¯Î¼ÎµÎ½Î± Î½Î± ÎµÎ¯Î½Î±Î¹ Ï„ÏŒÏƒÎ¿ Î´Î¹Î±ÏƒÎºÎµÎ´Î±ÏƒÏ„Î¹ÎºÎ®.",
        "Î”ÎµÎ½ Î¸Î± ÏƒÎ±Ï‚ Î±Ï€Î¿Î³Î¿Î·Ï„ÎµÏÏƒÎµÎ¹, ÎµÎ¯Î½Î±Î¹ ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ®.",

        # ÎŸÏ…Î´Î­Ï„ÎµÏÎµÏ‚ ÎºÏÎ¹Ï„Î¹ÎºÎ­Ï‚ Î¼Îµ Î¬ÏÎ½Î·ÏƒÎ· (2.3 Î±ÏƒÏ„Î­ÏÎ¹Î±)
        "Î”ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÎºÎ±ÎºÎ® Ï„Î±Î¹Î½Î¯Î±, Î±Î»Î»Î¬ Î¿ÏÏ„Îµ ÎµÎ½Ï„Ï…Ï€Ï‰ÏƒÎ¹Î±ÎºÎ®.",
        "Î”ÎµÎ½ Î¼Îµ Î±Ï€Î¿Î³Î¿Î®Ï„ÎµÏ…ÏƒÎµ, Î±Î»Î»Î¬ Î´ÎµÎ½ Î¼Îµ ÎµÎ½Î¸Î¿Ï…ÏƒÎ¯Î±ÏƒÎµ ÎºÎ¹ÏŒÎ»Î±Ï‚.",
        "Î”ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Ï„ÏŒÏƒÎ¿ Î¬ÏƒÏ‡Î·Î¼Î· ÏŒÏƒÎ¿ Ï€ÎµÏÎ¯Î¼ÎµÎ½Î±.",
        "Î”ÎµÎ½ Î¸Î± Î­Î»ÎµÎ³Î± ÏŒÏ„Î¹ ÎµÎ¯Î½Î±Î¹ Ï‡Î¬ÏƒÎ¹Î¼Î¿ Ï‡ÏÏŒÎ½Î¿Ï….",
        "Î”ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î· ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î´Î¿Ï…Î»ÎµÎ¹Î¬ Ï„Î¿Ï… ÏƒÎºÎ·Î½Î¿Î¸Î­Ï„Î·, Î±Î»Î»Î¬ Î¿ÏÏ„Îµ Î· Ï‡ÎµÎ¹ÏÏŒÏ„ÎµÏÎ·.",
        "Î”ÎµÎ½ Î¼Ï€Î¿ÏÏ Î½Î± Î²ÏÏ‰ ÎºÎ¬Ï„Î¹ Î±ÏÎ½Î·Ï„Î¹ÎºÏŒ ÏƒÎµ Î±Ï…Ï„Î® Ï„Î·Î½ Ï„Î±Î¹Î½Î¯Î±.",

        # Î‘ÏÎ½Î·Ï„Î¹ÎºÎ­Ï‚ ÎºÏÎ¹Ï„Î¹ÎºÎ­Ï‚ Î¼Îµ Î¬ÏÎ½Î·ÏƒÎ· (1 Î±ÏƒÏ„Î­ÏÎ¹)
        "Î”ÎµÎ½ Î¼Ï€Î¿ÏÏ Î½Î± Ï€Ï‰ ÎºÎ±Î»Î¬ Î»ÏŒÎ³Î¹Î± Î³Î¹Î± Î±Ï…Ï„Î® Ï„Î·Î½ Ï„Î±Î¹Î½Î¯Î±.",
        "Î”ÎµÎ½ Ï€ÏÎ¿ÏƒÏ†Î­ÏÎµÎ¹ Ï„Î¯Ï€Î¿Ï„Î± Î±Î¾Î¹ÏŒÎ»Î¿Î³Î¿.",
        "Î”ÎµÎ½ Î±Î¾Î¯Î¶ÎµÎ¹ Ï„Î¿Î½ Ï‡ÏÏŒÎ½Î¿ ÏƒÎ±Ï‚.",

        # Î‘ÏÎ½Î·Ï„Î¹ÎºÎ­Ï‚ ÎºÏÎ¹Ï„Î¹ÎºÎ­Ï‚ Î¼Îµ Î¬ÏÎ½Î·ÏƒÎ· (0 Î±ÏƒÏ„Î­ÏÎ¹)
        "Î”ÎµÎ½ Î±Î¾Î¯Î¶ÎµÎ¹ Ï„Î± Ï‡ÏÎ®Î¼Î±Ï„Î¬ ÏƒÎ¿Ï…, ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÎºÎ±Î»Î®.",
        "Î”ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎ± Î½Î± Î´Ï‰ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿ Î±Ï€ÏŒ 20 Î»ÎµÏ€Ï„Î¬, Ï„ÏŒÏƒÎ¿ ÎºÎ±ÎºÎ®.",
        "Î”ÎµÎ½ Ï€ÏÏŒÎºÎµÎ¹Ï„Î±Î¹ Î½Î± ÏƒÏ…ÏƒÏ„Î®ÏƒÏ‰ Î±Ï…Ï„Î® Ï„Î·Î½ Ï„Î±Î¹Î½Î¯Î± ÏƒÎµ ÎºÎ±Î½Î­Î½Î±Î½.",
        "Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Ï‡ÎµÎ¹ÏÏŒÏ„ÎµÏÎ· Ï„Î±Î¹Î½Î¯Î± Ï†Î­Ï„Î¿Ï‚."
    ],
    "stars": [
        # Î˜ÎµÏ„Î¹ÎºÎ­Ï‚ (5 Î±ÏƒÏ„Î­ÏÎ¹Î±)
        5, 5, 5, 5,

        # Î˜ÎµÏ„Î¹ÎºÎ­Ï‚ (4 Î±ÏƒÏ„Î­ÏÎ¹Î±)
        4, 4, 4,

        # ÎŸÏ…Î´Î­Ï„ÎµÏÎµÏ‚ (2,3 Î±ÏƒÏ„Î­ÏÎ¹Î±)
        3, 3, 3, 2, 2,3,

        # Î‘ÏÎ½Î·Ï„Î¹ÎºÎ­Ï‚ (1 Î±ÏƒÏ„Î­ÏÎ¹Î±)
        1, 1, 1,

        # Î‘ÏÎ½Î·Ï„Î¹ÎºÎ­Ï‚ (0 Î±ÏƒÏ„Î­ÏÎ¹)
        0, 0, 0, 0
    ]
}

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± dataframe Î¼Îµ Ï„Î± Î½Î­Î± Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î±
negation_df = pd.DataFrame(negation_data)

# Î ÏÎ¿Î²Î¿Î»Î® Ï„Ï‰Î½ Î½Î­Ï‰Î½ Ï€Î±ÏÎ±Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½
print(negation_df.head())
print(f"Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î½Î­Ï‰Î½ Ï€Î±ÏÎ±Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½: {len(negation_df)}")

# Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· Î¼Îµ Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ dataframe
balanced_df_new = pd.concat([balanced_df, negation_df], ignore_index=True)

# Î‘Î½Î±ÎºÎ¬Ï„ÎµÎ¼Î± Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
balanced_df_new = balanced_df_new.sample(frac=1).reset_index(drop=True)
# Î ÏÎ¿Î²Î¿Î»Î® ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ Î³Î¹Î± Ï„Î¹Ï‚ Ï„Î¹Î¼Î­Ï‚ Ï„Ï‰Î½ stars
print(balanced_df_new['stars'].value_counts().sort_index())

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î½Î­Î¿Ï… ÏƒÏ…Î½ÏŒÎ»Î¿Ï… Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
balanced_df_new.to_csv("balanced_dataset_new.csv", index=False)

def sentiment_category_3(stars):
  if stars <= 1:
    return 0  # Î‘ÏÎ½Î·Ï„Î¹ÎºÏŒ
  elif stars <= 3:
    return 1  # ÎŸÏ…Î´Î­Ï„ÎµÏÎ¿
  else:
    return 2  # Î˜ÎµÏ„Î¹ÎºÏŒ

balanced_df_new['sentiment'] = balanced_df_new['stars'].apply(sentiment_category_3)

balanced_df_new.head()

plt.figure(figsize=(10, 6))
sns.countplot(x='sentiment', data=balanced_df_new,palette="viridis")
plt.title('ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎšÏÎ¹Ï„Î¹ÎºÏÎ½ Î¤Î±Î¹Î½Î¹ÏÎ½')
plt.xticks([0, 1, 2], ["Î‘ÏÎ½Î·Ï„Î¹ÎºÎ®", "ÎŸÏ…Î´Î­Ï„ÎµÏÎ·", "Î˜ÎµÏ„Î¹ÎºÎ®"], fontsize=12)
plt.xlabel('ÎšÏÎ¹Ï„Î¹ÎºÎ®')
plt.ylabel('Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î£Ï‡Î¿Î»Î¯Ï‰Î½')
plt.show()

# Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÎµ train ÎºÎ±Î¹ test sets
X_train, X_test, y_train, y_test = train_test_split(
    balanced_df_new['review'],
    balanced_df_new['sentiment'],
    test_size=0.2,
    random_state=42
)

def install_spacy_greek_model():
    print("Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Î¿Ï… ÎµÎ»Î»Î·Î½Î¹ÎºÎ¿Ï Î¼Î¿Î½Ï„Î­Î»Î¿Ï… spaCy...")
    try:
        subprocess.check_call([sys.executable, "-m", "spacy", "download", "el_core_news_sm"])
        print("Î¤Î¿ ÎµÎ»Î»Î·Î½Î¹ÎºÏŒ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î¬Î¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
    except subprocess.CalledProcessError:
        print("Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÎ³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï….")
        return False
    return True

if __name__ == "__main__":
    install_spacy_greek_model()

nltk.download('stopwords')

greek_stopwords = set(stopwords.words('greek'))

print(f"Î•Î»Î»Î·Î½Î¹ÎºÎ¬ stopwords: {sorted(greek_stopwords)}")

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… ÎµÎ»Î»Î·Î½Î¹ÎºÎ¿Ï Î¼Î¿Î½Ï„Î­Î»Î¿Ï… spaCy

nlp = spacy.load("el_core_news_sm")
def preprocess_text(text, use_spacy=True):
    if not isinstance(text, str):
        return ""

    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ Ï€ÎµÎ¶Î¬
    text = text.lower()

    # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· ÎµÎ¹Î´Î¹ÎºÏÎ½ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÏ‰Î½ ÎºÎ±Î¹ Î±ÏÎ¹Î¸Î¼ÏÎ½
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)

    if use_spacy and nlp:
        # Î§ÏÎ®ÏƒÎ· spaCy Î³Î¹Î± lemmatization
        doc = nlp(text)
        text = " ".join([token.lemma_ for token in doc if token.text not in greek_stopwords])
    else:
        # Î‘Ï€Î»Î® Î±Ï†Î±Î¯ÏÎµÏƒÎ· stopwords
        text = " ".join([word for word in text.split() if word not in greek_stopwords])

    return text

# Î•Ï†Î±ÏÎ¼Î¿Î³Î® Ï€ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚
X_train_processed = X_train.apply(preprocess_text)
X_test_processed = X_test.apply(preprocess_text)

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± TF-IDF features
tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train_processed)
X_test_tfidf = tfidf.transform(X_test_processed)

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, C=1, solver='liblinear'),
    "Linear SVM": LinearSVC(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Naive Bayes": MultinomialNB()
}

results = {}

for name, model in models.items():
    print(f"Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… {name}...")
    model.fit(X_train_tfidf, y_train)

# Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·
    y_pred = model.predict(X_test_tfidf)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'report': report
    }

    print(f"{name} - Î‘ÎºÏÎ¯Î²ÎµÎ¹Î±: {accuracy:.4f}")
    print(report)
    print("-" * 60)

# Î•ÏÏÎµÏƒÎ· Ï„Î¿Ï… ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
best_model = results[best_model_name]['model']
print(f"Î¤Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ¯Î½Î±Î¹: {best_model_name} Î¼Îµ Î±ÎºÏÎ¯Î²ÎµÎ¹Î± {results[best_model_name]['accuracy']:.4f}")

# Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± BERT
# ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Ï‰Î½ dataframes ÏƒÎµ datasets
train_dataset = Dataset.from_dict({
    'text': X_train.tolist(),
    'label': y_train.tolist()
})

test_dataset = Dataset.from_dict({
    'text': X_test.tolist(),
    'label': y_test.tolist()
})

# Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Ï Ï„Î¿ Greek BERT
model_name = "nlpaueb/bert-base-greek-uncased-v1"

tokenizer = AutoTokenizer.from_pretrained(model_name)

#  Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î¼Îµ tokenizer

def preprocess_function(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        padding='max_length',
        max_length=512  # Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î¼Î®ÎºÎ¿Ï…Ï‚ ÏƒÏ„Î± 512 tokens
    )

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_test = test_dataset.map(preprocess_function, batched=True)

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î¿Ï Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = accuracy_score(labels, predictions)
    report = classification_report(labels, predictions, output_dict=True)
    return {
        'accuracy': accuracy,
        'f1_macro': report['macro avg']['f1-score'],
        'precision': report['macro avg']['precision'],
        'recall': report['macro avg']['recall'],
        **{f'f1_{i}': report[str(i)]['f1-score'] for i in range(3)}
    }

#  Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
num_labels = len(np.unique(y_train))
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

training_args = TrainingArguments(
    output_dir="./greek-sentiment-model",
    run_name="greek-sentiment-experiment",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·
trainer.train()

# Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Î¼Îµ Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ­Ï‚ Î¼ÎµÏ„ÏÎ¹ÎºÎ­Ï‚
results = trainer.evaluate()
print(f"Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚: {results}")
# Î Î±ÏÎ±Î³Ï‰Î³Î® ÎºÎ±Î¹ ÎµÎºÏ„ÏÏ€Ï‰ÏƒÎ· Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ¿Ï classification report
predictions = trainer.predict(tokenized_test)
preds = np.argmax(predictions.predictions, axis=-1)
print(classification_report(y_test, preds, target_names=['Î‘ÏÎ½Î·Ï„Î¹ÎºÎ®', 'ÎŸÏ…Î´Î­Ï„ÎµÏÎ·', 'Î˜ÎµÏ„Î¹ÎºÎ®']))
# ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· confusion matrix
cm = confusion_matrix(y_test, preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Î‘ÏÎ½Î·Ï„Î¹ÎºÎ®', 'ÎŸÏ…Î´Î­Ï„ÎµÏÎ·', 'Î˜ÎµÏ„Î¹ÎºÎ®'],
            yticklabels=['Î‘ÏÎ½Î·Ï„Î¹ÎºÎ®', 'ÎŸÏ…Î´Î­Ï„ÎµÏÎ·', 'Î˜ÎµÏ„Î¹ÎºÎ®'])
plt.xlabel('Î ÏÎ¿Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î· Î•Ï„Î¹ÎºÎ­Ï„Î±')
plt.ylabel('Î ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ® Î•Ï„Î¹ÎºÎ­Ï„Î±')
plt.title('Confusion Matrix Î³Î¹Î± Ï„Î¿ BERT ÎœÎ¿Î½Ï„Î­Î»Î¿')
plt.tight_layout()
plt.show()

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
model.save_pretrained("./greek-sentiment-final-model")
tokenizer.save_pretrained("./greek-sentiment-final-model")

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Ï Î­Î½Î± Ï†Î¬ÎºÎµÎ»Î¿ ÏƒÏ„Î¿ Drive (Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹)
!mkdir -p "/content/drive/My Drive/ML_Models_final"

# Î‘Î½Ï„Î¹Î³ÏÎ¬Ï†Ï‰ Ï„Î¿ Ï†Î¬ÎºÎµÎ»Î¿ Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… ÏƒÏ„Î¿ Drive
!cp -r /content/greek-sentiment-final-model "/content/drive/My Drive/ML_Models_final/"

def predict_sentiment(text, model_type='bert'):
    # Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±
    processed_text = preprocess_text(text)

    if model_type == 'classic':
        # TF-IDF ÎºÎ±Î¹ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· Î¼Îµ Ï„Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ ÎºÎ»Î±ÏƒÎ¹ÎºÏŒ Î¼Î¿Î½Ï„Î­Î»Î¿
        text_tfidf = tfidf.transform([processed_text])
        prediction = best_model.predict(text_tfidf)[0]

        # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Î·Ï‚ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚ ÏƒÎµ ÎµÏ„Î¹ÎºÎ­Ï„Î±
        sentiment_label = ['Î‘ÏÎ½Î·Ï„Î¹ÎºÎ® ğŸ˜”', 'ÎŸÏ…Î´Î­Ï„ÎµÏÎ· ğŸ˜', 'Î˜ÎµÏ„Î¹ÎºÎ® ğŸ˜Š'][prediction]
        return sentiment_label

    elif model_type == 'bert':
        # Î§ÏÎ®ÏƒÎ· Ï„Î¿Ï… BERT Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
        inputs = tokenizer(processed_text, return_tensors="pt", truncation=True, padding=True)
        # ÎœÎµÏ„Î±ÎºÎ¯Î½Î·ÏƒÎ· Ï„Ï‰Î½ input tensors ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± ÏƒÏ…ÏƒÎºÎµÏ…Î® Î¼Îµ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
        inputs = {key: val.to(model.device) for key, val in inputs.items()}

        # Î ÏÏŒÎ²Î»ÎµÏˆÎ·
        with torch.no_grad():
            outputs = model(**inputs)
            prediction = torch.argmax(outputs.logits, dim=1).item()
            probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)
            confidence = probabilities[0][prediction].item() * 100

    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Î·Ï‚ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚ ÏƒÎµ ÎµÏ„Î¹ÎºÎ­Ï„Î±
    sentiment_labels =  ['Î‘ÏÎ½Î·Ï„Î¹ÎºÎ® ğŸ˜”', 'ÎŸÏ…Î´Î­Ï„ÎµÏÎ· ğŸ˜', 'Î˜ÎµÏ„Î¹ÎºÎ® ğŸ˜Š']
    result = f"{sentiment_labels[prediction]} (Î’ÎµÎ²Î±Î¹ÏŒÏ„Î·Ï„Î±: {confidence:.1f}%)"
    return result

# Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Ï‡ÏÎ®ÏƒÎ·Ï‚
sample_reviews = [
    "Î— Ï„Î±Î¹Î½Î¯Î± Î®Ï„Î±Î½ ÎµÏ„ÏƒÎ¹ ÎºÎ±Î¹ ÎµÏ„ÏƒÎ¹ Î±Î»Î»Î± Î¸Î± Ï„Î·Î½ Î¾Î±Î½Î±ÎµÎ²Î»ÎµÏ€Î±",
    "ÎœÎ­Ï„ÏÎ¹Î± Ï„Î±Î¹Î½Î¯Î±, ÎµÎ¯Ï‡Îµ ÎºÎ¬Ï€Î¿Î¹ÎµÏ‚ ÎºÎ±Î»Î­Ï‚ ÏƒÏ„Î¹Î³Î¼Î­Ï‚ Î±Î»Î»Î¬ Î³ÎµÎ½Î¹ÎºÎ¬ Î®Ï„Î±Î½ Î²Î±ÏÎµÏ„Î®.",
    "Î¤ÎµÎ»ÎµÎ¯Ï‰Ï‚ Ï‡Î¬Î»Î¹Î±, Ï‡Î¬ÏƒÎ¹Î¼Î¿ Ï‡ÏÏŒÎ½Î¿Ï… ÎºÎ±Î¹ Ï‡ÏÎ·Î¼Î¬Ï„Ï‰Î½."
]

print("Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î¼Îµ Ï„Î¿ ÎšÎ»Î±ÏƒÎ¹ÎºÏŒ ÎœÎ¿Î½Ï„Î­Î»Î¿:")
for review in sample_reviews:
    sentiment = predict_sentiment(review, 'classic')
    print(f"ÎšÎµÎ¯Î¼ÎµÎ½Î¿: {review}\nÎ£Ï…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±: {sentiment}\n")

print("\nÎ ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ Î¼Îµ Ï„Î¿ BERT ÎœÎ¿Î½Ï„Î­Î»Î¿:")
for review in sample_reviews:
    sentiment = predict_sentiment(review, 'bert')
    print(f"ÎšÎµÎ¯Î¼ÎµÎ½Î¿: {review}\nÎ£Ï…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±: {sentiment}\n")

"""**Gradio**"""

# Î¦Î¿ÏÏ„ÏÎ½Ï‰ Ï„Î¿Ï€Î¹ÎºÎ® ÎµÎ¹ÎºÏŒÎ½Î± ÏƒÎµ base64
with open("/content/hidden_figures.jpeg", "rb") as f:
    data = f.read()
data_url = "data:image/jpeg;base64," + base64.b64encode(data).decode()

# Î¦Ï„Î¹Î¬Ï‡Î½Ï‰ Ï„Î¿ HTML Î³Î¹Î± Ï„Î·Î½ ÎµÎ¹ÎºÏŒÎ½Î±
image_html = f"""
<div style="text-align:center; margin:20px 0;">
  <img src="{data_url}"
       alt="Movie Poster"
       style="max-width:90%; max-height:350px; object-fit:contain; display:inline-block;"
  />
</div>
"""
# ÎšÎ±Î¸Î±ÏÎ¯Î¶ÎµÎ¹ Ï„Î¹Ï‚ Ï„Î¹Î¼Î­Ï‚ Î³Î¹Î± Ï„Î¿ review ÎºÎ±Î¹ Ï„Î¿ sentiment
def clear_fields():
    return "", ""

#  Blocks Î³Î¹Î± Î½Î± Ï„Î¿Ï€Î¿Î¸ÎµÏ„Î®ÏƒÏ‰ Ï„Î± components Î¼Îµ ÏŒÏ€Î¿Î¹Î± ÏƒÎµÎ¹ÏÎ¬ Î¸Î­Î»Ï‰
with gr.Blocks(title="Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î®Î¼Î±Ï„Î¿Ï‚") as demo:
    # Î¤Î¯Ï„Î»Î¿Ï‚
    gr.Markdown(""" # ğŸ¬ Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î®Î¼Î±Ï„Î¿Ï‚ ÎšÏÎ¹Ï„Î¹ÎºÎ®Ï‚ Î¤Î±Î¹Î½Î¯Î±Ï‚ """)
    # Î ÎµÏÎ¹Î³ÏÎ±Ï†Î®
    gr.Markdown("""*Î“ÏÎ¬ÏˆÎµ Î¼Î¹Î± ÎºÏÎ¹Ï„Î¹ÎºÎ® Î³Î¹Î± Î¼Î¹Î± Ï„Î±Î¹Î½Î¯Î± ÎºÎ±Î¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î¼Î±Ï‚ Î¸Î± Î±Î½Î±Î»ÏÏƒÎµÎ¹ Ï„Î¿ ÏƒÏ…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î± Ï„Î¿Ï… ÎºÎµÎ¹Î¼Î­Î½Î¿Ï….*""")
    #  HTML Î¼Îµ Ï„Î·Î½ ÎºÎµÎ½Ï„ÏÎ±ÏÎ¹ÏƒÎ¼Î­Î½Î· ÎµÎ¹ÎºÏŒÎ½Î±
    gr.HTML(image_html)

    # Î¤Î¿Ï€Î¿Î¸ÎµÏ„Ï Ï„Î¿ textbox ÎºÎ±Î¹ Ï„Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± ÏƒÎµ Î¼Î¯Î± ÏƒÎµÎ¹ÏÎ¬ (Î´Î¯Ï€Î»Î±-Î´Î¯Ï€Î»Î±)
    with gr.Row():
        # Î‘ÏÎ¹ÏƒÏ„ÎµÏÎ® ÏƒÏ„Î®Î»Î·: Textbox ÎµÎ¹ÏƒÎ±Î³Ï‰Î³Î®Ï‚
        review = gr.Textbox(
            lines=4,
            placeholder="Î“ÏÎ¬ÏˆÎµ ÎµÎ´Ï Ï„Î·Î½ ÎºÏÎ¹Ï„Î¹ÎºÎ® ÏƒÎ¿Ï… Î³Î¹Î± Ï„Î·Î½ Ï„Î±Î¹Î½Î¯Î±...",
            label="ÎšÏÎ¹Ï„Î¹ÎºÎ®",

        )

        # Î”ÎµÎ¾Î¹Î¬ ÏƒÏ„Î®Î»Î·: Î ÎµÎ´Î¯Î¿ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î¿Ï‚
        sentiment = gr.Textbox(
            label="Î‘Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±",
            lines=4,
        )


    review.submit(predict_sentiment, inputs=review, outputs=sentiment)

    # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± row Î³Î¹Î± Ï„Î± ÎºÎ¿Ï…Î¼Ï€Î¹Î¬ ÏÏƒÏ„Îµ Î½Î± ÎµÎ¼Ï†Î±Î½Î¯Î¶Î¿Î½Ï„Î±Î¹ Î´Î¯Ï€Î»Î±-Î´Î¯Ï€Î»Î±
    with gr.Row():
        # ÎšÎ¿Ï…Î¼Ï€Î¯ Î‘Î½Î¬Î»Ï…ÏƒÎ·
        analyze_btn = gr.Button("Î‘Î½Î¬Î»Ï…ÏƒÎ· ğŸ”", variant="primary")

        # ÎšÎ¿Ï…Î¼Ï€Î¯ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚
        clear_btn = gr.Button("ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ğŸ—‘ï¸", variant="secondary")

    # Î£Ï…Î½Î´Î­Ï‰ Ï„Î¹Ï‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯ÎµÏ‚ Î¼Îµ Ï„Î± ÎºÎ¿Ï…Î¼Ï€Î¹Î¬
    analyze_btn.click(predict_sentiment, inputs=review, outputs=sentiment)
    clear_btn.click(clear_fields, inputs=None, outputs=[review, sentiment])

demo.launch(share=True)

"""**ÎŸÎ Î¤Î™ÎšÎŸÎ ÎŸÎ™Î—Î£Î•Î™Î£**

**Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± FlowChart**


dot = Digraph(comment='flowchart')

# Î£Ï„Î¬Î´Î¹Î±
dot.node('A', 'Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® Î’Î¹Î²Î»Î¹Î¿Î¸Î·ÎºÏÎ½ & Mount Google Drive', shape='box', style='filled', fillcolor='#E0F7FA')
dot.node('B', 'Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Dataset', shape='box', style='filled', fillcolor='#FFECB3')
dot.node('C', 'ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½', shape='box', style='filled', fillcolor='#FFE0B2')
dot.node('D', 'Î”ÎµÎ¹Î³Î¼Î±Ï„Î¿Î»Î·ÏˆÎ¯Î± Î³Î¹Î± Î¯ÏƒÎ¿ Î±ÏÎ¹Î¸Î¼ÏŒ Î±Î½Î¬ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±', shape='box', style='filled', fillcolor='#C8E6C9')
dot.node('E', 'Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· ÎšÏÎ¹Ï„Î¹ÎºÏÎ½ Î¼Îµ Î†ÏÎ½Î·ÏƒÎ· (Negation)', shape='box', style='filled', fillcolor='#D1C4E9')
dot.node('F', 'ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® "stars" ÏƒÎµ Î¤ÏÎµÎ¹Ï‚ ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚ Sentiment', shape='box', style='filled', fillcolor='#B3E5FC')
dot.node('G', 'Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎšÎµÎ¹Î¼Î­Î½Î¿Ï…', shape='box', style='filled', fillcolor='#FFCDD2')
dot.node('H', 'TF-IDF Vectorization', shape='box', style='filled', fillcolor='#F0F4C3')
dot.node('I', 'Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÎšÎ»Î±ÏƒÎ¹ÎºÏÎ½ ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ ML', shape='box', style='filled', fillcolor='#E0F7FA')
dot.node('J', 'Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Ï‰Î½ & Î•Ï€Î¹Î»Î¿Î³Î® ÎšÎ±Î»ÏÏ„ÎµÏÎ¿Ï…', shape='box', style='filled', fillcolor='#FFECB3')
dot.node('K', 'Fine-tuning Greek BERT ', shape='box', style='filled', fillcolor='#FFE0B2')
dot.node('L', 'Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· BERT', shape='box', style='filled', fillcolor='#C8E6C9')

# Î£ÏÎ½Î´ÎµÏƒÎ· Ï„Ï‰Î½ ÎºÏŒÎ¼Î²Ï‰Î½
dot.edges(['AB', 'BC', 'CD', 'DE', 'EF', 'FG', 'GH','HI','IJ','JK','KL'])

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î¿ PNG
dot.render('project_based_learning_flowchart', format='png', cleanup=True)

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· ÏƒÎµ Jupyter Notebook (Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹)
dot.view()

"""**Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± wordcloud**"""

# Function to clean text for word cloud
def clean_for_wordcloud(text):
    if not isinstance(text, str):
        return ""

    # Convert to lowercase
    text = text.lower()

    # Remove URLs, special characters, numbers
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', ' ', text)

    # Remove short words
    words = [word for word in text.split() if len(word) > 2]

    # Remove stopwords
    words = [word for word in words if word not in greek_stopwords]

    return " ".join(words)

# Group texts by sentiment
sentiment_texts = {
    0: [],  # Negative
    1: [],  # Neutral
    2: []   # Positive
}

# Collect all texts by sentiment category
for idx, row in balanced_df_new.iterrows():
    sentiment_texts[row['sentiment']].append(row['review'])

# Clean texts and get word counts for each sentiment
cleaned_texts = {}
word_counts_by_sentiment = {}

for sentiment, texts in sentiment_texts.items():
    # Clean each text
    cleaned = [clean_for_wordcloud(text) for text in texts]
    cleaned_texts[sentiment] = " ".join(cleaned)

    # Count words
    word_counts_by_sentiment[sentiment] = Counter(cleaned_texts[sentiment].split())
    print(f"Total unique words for sentiment {sentiment}: {len(word_counts_by_sentiment[sentiment])}")

# Find common words across all sentiment categories
# Get sets of words from each sentiment that appear more than a threshold
threshold = 5  # Minimum count to consider a word
word_sets = {}
for sentiment, counter in word_counts_by_sentiment.items():
    word_sets[sentiment] = {word for word, count in counter.items() if count > threshold}

# Find words that appear in all sentiment categories
common_words = set.intersection(*word_sets.values())
print(f"\nFound {len(common_words)} common words across all sentiment categories")
print(f"Sample common words: {list(common_words)[:20]}")

# Remove common words from each sentiment's text
filtered_texts = {}
for sentiment, text in cleaned_texts.items():
    words = text.split()
    filtered_words = [word for word in words if word not in common_words]
    filtered_texts[sentiment] = " ".join(filtered_words)

    # Count top words after filtering
    word_counts = Counter(filtered_words)
    print(f"\nTop 15 distinctive words for sentiment {sentiment}: {word_counts.most_common(15)}")

# Create a figure with subplots for each sentiment
fig, axes = plt.subplots(1, 3, figsize=(20, 8))
sentiment_names = {0: 'Î‘ÏÎ½Î·Ï„Î¹ÎºÏŒ ğŸ˜”', 1: 'ÎŸÏ…Î´Î­Ï„ÎµÏÎ¿ ğŸ˜', 2: 'Î˜ÎµÏ„Î¹ÎºÏŒ ğŸ˜Š'}

# Create word cloud for each sentiment with filtered texts
for sentiment, text in filtered_texts.items():
    if not text.strip():  # Check if text is empty after filtering
        print(f"Warning: No words left for sentiment {sentiment} after filtering!")
        continue

    # Create word cloud
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        max_words=100,
        contour_width=3,
        contour_color='steelblue'
    ).generate(text)

    # Plot
    ax = axes[sentiment]
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.set_title(f'Distinctive Words: {sentiment_names[sentiment]}', fontsize=16)
    ax.axis('off')

plt.tight_layout()
plt.savefig('wordcloud_distinctive.png', dpi=300)
plt.show()

# Also generate standard word clouds without common word filtering for comparison
fig, axes = plt.subplots(1, 3, figsize=(20, 8))

for sentiment, text in cleaned_texts.items():
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        max_words=100,
        contour_width=3,
        contour_color='steelblue'
    ).generate(text)

    ax = axes[sentiment]
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.set_title(f'All Words: {sentiment_names[sentiment]}', fontsize=16)
    ax.axis('off')

plt.tight_layout()
plt.savefig('wordcloud_all.png', dpi=300)
plt.show()
